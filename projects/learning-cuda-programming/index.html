<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>cuda-programming | Suraj Anand</title> <meta name="author" content="Suraj Anand"> <meta name="description" content="from cpu optimizations to gpu kernels"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?505194d008ab58abcc3c0db57c2ae876"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://surajk610.github.io/projects/learning-cuda-programming/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Suraj Anand</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">cuda-programming</h1> <p class="post-description">from cpu optimizations to gpu kernels</p> </header> <article> <h1 id="from-cpu-optimizations-to-gpu-kernels">From CPU Optimizations to GPU Kernels</h1> <p>–</p> <h2 id="introduction">Introduction</h2> <p>This blog post describes my journey through Parallel Computing on Heterogeneous (CPU + GPU) Systems at Brown University. We progress from basic CPU optimizations to advanced GPU programming, exploring the fundamental concepts that make modern high-performance computing possible. What started as simple matrix operations evolved into a deep understanding of hardware-software co-design, memory hierarchies, and parallel algorithm optimization.</p> <h2 id="task-1-cpu-matrix-vector-multiplication">Task 1: CPU Matrix-Vector Multiplication</h2> <h3 id="the-foundation-understanding-memory-hierarchy-and-roofline-models">The Foundation: Understanding Memory Hierarchy and Roofline Models</h3> <p>We began with the simple task of matrix-vector multiplication. Much of the story behind this class is about maximizing performance for the given hardware. Memory access patterns and theoretical performance bounds can make or break performance.</p> <h4 id="roofline-model-analysis">Roofline Model Analysis</h4> <p>Before diving into optimizations, let’s go over the roofline model, a crucial framework for understanding performance limitations. The roofline model essentially has two zones: a memory bound zone and a compute bound zone. You would like to be in the compute bound zone in order to maximally utilize your given hardware budget.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/cuda_programming/gpu_roofline_model-480.webp 480w, /assets/img/cuda_programming/gpu_roofline_model-800.webp 800w, /assets/img/cuda_programming/gpu_roofline_model-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/cuda_programming/gpu_roofline_model.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="nsight systems" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Here's the naive GPU roofline model for a RTX 6000. In practice, the *blockDim* causes waves and there are weird interaction effects with things such as memory coalescing. </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/cuda_programming/gpu_roofline_model-480.webp 480w, /assets/img/cuda_programming/gpu_roofline_model-800.webp 800w, /assets/img/cuda_programming/gpu_roofline_model-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/cuda_programming/gpu_roofline_model.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Here's the naive GPU roofline model for a RTX 6000. In practice, the *blockDim* causes waves and there are weird interaction effects with things such as memory coalescing. </div> <p>For matrix-vector multiplication:</p> <ul> <li> <strong>FLOPS</strong>: 2MN operations (one multiply, one add per element)</li> <li> <strong>Memory Access</strong>: 3 loads + 1 store per operation = 32MN bytes (assuming 8-byte doubles)</li> <li> <strong>Arithmetic Intensity</strong>: 2MN FLOPS ÷ 32MN bytes = <strong>1/16 FLOPS/byte</strong> </li> </ul> <p>This low arithmetic intensity of 0.0625 placed my algorithm firmly on the memory-bound region of the roofline model, meaning performance would be limited by memory bandwidth rather than computational capacity.</p> <h4 id="baseline-implementation">Baseline Implementation</h4> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">double</span><span class="o">*</span> <span class="nf">performMatrixVectorMultiplication</span><span class="p">(</span><span class="kt">double</span><span class="o">**</span> <span class="n">matrix</span><span class="p">,</span> <span class="kt">double</span><span class="o">*</span> <span class="n">vector</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numRowsM</span><span class="p">,</span> <span class="kt">int</span> <span class="n">vecLength</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">double</span><span class="o">*</span> <span class="n">vectorOut</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">double</span><span class="p">[</span><span class="n">numRowsM</span><span class="p">];</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numRowsM</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">vectorOut</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">vecLength</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">vectorOut</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">vector</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="n">vectorOut</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>This naive implementation achieved an average FLOP rate of <strong>715.598 MFLOPS</strong> across various matrix sizes, serving as my performance baseline.</p> <h4 id="optimization-1-contiguous-memory-layout">Optimization 1: Contiguous Memory Layout</h4> <p>The first major lesson was about memory locality. Traditional 2D array allocation creates scattered memory chunks that destroy cache efficiency:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">double</span><span class="o">**</span> <span class="nf">generateRandomMatrixContiguous</span><span class="p">(</span><span class="kt">int</span> <span class="n">numRows</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numCols</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">double</span><span class="o">**</span> <span class="n">matrix</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">double</span><span class="o">*</span><span class="p">[</span><span class="n">numRows</span><span class="p">];</span>
  <span class="n">matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">double</span><span class="p">[</span><span class="n">numRows</span><span class="o">*</span><span class="n">numCols</span><span class="p">];</span>  <span class="c1">// Single allocation</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numRows</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">i</span><span class="o">*</span><span class="n">numCols</span><span class="p">;</span>      <span class="c1">// Pointer arithmetic</span>

  <span class="c1">// ... initialization code</span>
  <span class="k">return</span> <span class="n">matrix</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Surprising Result</strong>: Contiguous allocation actually slightly decreased performance to <strong>666.975 MFLOPS</strong>. This counterintuitive result taught me that hardware behavior isn’t always predictable and that measurement is crucial.</p> <h4 id="optimization-2-compiler-optimizations---the-game-changer">Optimization 2: Compiler Optimizations - The Game Changer</h4> <p>The most dramatic improvements came from compiler optimizations:</p> <table> <thead> <tr> <th>Optimization Level</th> <th>Average FLOP Rate</th> <th>Speedup</th> </tr> </thead> <tbody> <tr> <td>No optimization</td> <td>715 MFLOPS</td> <td>1.0x</td> </tr> <tr> <td>-O1</td> <td>941 MFLOPS</td> <td>1.3x</td> </tr> <tr> <td>-O2</td> <td>1,759 MFLOPS</td> <td>2.5x</td> </tr> <tr> <td>-O3</td> <td>1,807 MFLOPS</td> <td>2.5x</td> </tr> </tbody> </table> <p><strong>Key Insight</strong>: The jump from -O1 to -O2 nearly doubled performance, while -O2 to -O3 showed diminishing returns. This taught me that compiler optimizations often outperform manual optimizations and that understanding what the compiler does is crucial.</p> <h4 id="optimization-3-manual-loop-unrolling">Optimization 3: Manual Loop Unrolling</h4> <p>I implemented loop unrolling by factors of 2 and 4:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">double</span><span class="o">*</span> <span class="nf">performMatrixVectorMultiplicationUnrolled4</span><span class="p">(</span><span class="kt">double</span><span class="o">**</span> <span class="n">matrix</span><span class="p">,</span> <span class="kt">double</span><span class="o">*</span> <span class="n">vector</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numRowsM</span><span class="p">,</span> <span class="kt">int</span> <span class="n">vecLength</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">double</span><span class="o">*</span> <span class="n">vectorOut</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">double</span><span class="p">[</span><span class="n">numRowsM</span><span class="p">];</span>

  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numRowsM</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">vectorOut</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">&lt;</span> <span class="n">vecLength</span><span class="p">;</span> <span class="n">j</span> <span class="o">+=</span> <span class="mi">4</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">vectorOut</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">vector</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span>
                      <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">vector</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span>
                      <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">vector</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span>
                      <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="o">+</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">vector</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">3</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="c1">// Handle remaining elements</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">vecLength</span><span class="p">;</span> <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">vectorOut</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">matrix</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">vector</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
    <span class="p">}</span>
  <span class="p">}</span>

  <span class="k">return</span> <span class="n">vectorOut</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Performance Results</strong> (without compiler optimization):</p> <ul> <li>Unroll-by-2: <strong>864.688 MFLOPS</strong> (1.2x speedup)</li> <li>Unroll-by-4: <strong>986.188 MFLOPS</strong> (1.4x speedup)</li> </ul> <p><strong>Trade-off Discovery</strong>: Manual unrolling improved performance but made code less readable. More importantly, high compiler optimization levels often achieved better results than manual unrolling.</p> <h2 id="task-2-matrix-matrix-multiplication">Task 2: Matrix-Matrix Multiplication</h2> <p>Matrix-matrix multiplication is the core of most deep learning and scientific computing these days. Unfortunately matrix multiplication with <code class="language-plaintext highlighter-rouge">MN</code> and <code class="language-plaintext highlighter-rouge">NP</code> matrices is <code class="language-plaintext highlighter-rouge">2MNP</code> flops (N multiplications and N-1 additions per entry in the final matrix). With this <code class="language-plaintext highlighter-rouge">O(n^3)</code> complexity, we need more sophisticated optimization strategies. The roofline analysis revealed:</p> <ul> <li> <strong>FLOPS</strong>: 2NKM operations</li> <li> <strong>Memory Access</strong>: 3NKM loads/stores × 8 bytes = 24NKM bytes</li> <li> <strong>Arithmetic Intensity</strong>: 2NKM ÷ 24NKM = <strong>1/12 FLOPS/byte</strong> </li> </ul> <p>Still memory-bound, but slightly better than matrix-vector multiplication.</p> <h4 id="loop-ordering-matters">Loop Ordering Matters</h4> <p>One of the most surprising discoveries was how much loop order affects performance. I tested four different orderings:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Version 1: i-j-k order (cache-unfriendly)</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">];</span>  <span class="c1">// B accessed column-wise (non-contiguous)</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// Version 2: i-k-j order (cache-friendly)  </span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">];</span>  <span class="c1">// B accessed row-wise (contiguous)</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Performance Comparison</strong>: | Loop Order | Average FLOP Rate | Performance Gain | |————|——————|——————| | i-j-k NMK | 0.487667 TFLOPS | Baseline | | m-n-k MNK | 0.482222 TFLOPS | -1.1% | | i-k-j NKM | 0.524519 TFLOPS | +7.5% | | k-n-m KNM | 0.524741 TFLOPS | +7.6% |</p> <p><strong>Key Insight</strong>: The i-k-j and k-n-m orders performed best because they access matrix B row-wise instead of column-wise, dramatically improving cache locality. This single change improved performance by ~7.5% without any algorithmic modifications.</p> <h4 id="introduction-to-parallel-computing-with-openmp">Introduction to Parallel Computing with OpenMP</h4> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">double</span><span class="o">**</span> <span class="nf">parallelMatrixMatrixMultiplication</span><span class="p">(</span><span class="kt">double</span><span class="o">**</span> <span class="n">C</span><span class="p">,</span> <span class="kt">double</span><span class="o">**</span> <span class="n">A</span><span class="p">,</span> <span class="kt">double</span><span class="o">**</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">;</span>
  <span class="cp">#pragma omp parallel for private(i, j, k) shared(A, B, C) collapse(2)
</span>  <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">C</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">];</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">C</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Breakthrough Results</strong>: OpenMP parallelization with <code class="language-plaintext highlighter-rouge">-O0</code> compilation achieved <strong>~1.001 TFLOPS</strong>, roughly <strong>doubling</strong> the baseline performance.</p> <p><strong>Key Concepts Learned</strong>:</p> <ul> <li>The <code class="language-plaintext highlighter-rouge">collapse(2)</code> directive creates a larger iteration space by combining the i and j loops, enabling better load balancing across threads</li> <li>Understanding shared vs. private variables is crucial for correctness</li> <li>Thread-level parallelism can dramatically improve performance even on memory-bound algorithms</li> </ul> <h4 id="cache-blocking-optimization-mixed-results">Cache-Blocking Optimization: Mixed Results</h4> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">double</span><span class="o">**</span> <span class="nf">loopBlockingMatrixMatrixMultiplication</span><span class="p">(</span><span class="kt">double</span><span class="o">**</span> <span class="n">C</span><span class="p">,</span> <span class="kt">double</span><span class="o">**</span> <span class="n">A</span><span class="p">,</span> <span class="kt">double</span><span class="o">**</span> <span class="n">B</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="p">,</span> <span class="kt">int</span> <span class="n">blockSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">blockSize</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">j</span> <span class="o">+=</span> <span class="n">blockSize</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">k</span> <span class="o">+=</span> <span class="n">blockSize</span><span class="p">)</span> <span class="p">{</span>
                <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">ii</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span> <span class="n">ii</span> <span class="o">&lt;</span> <span class="n">i</span> <span class="o">+</span> <span class="n">blockSize</span> <span class="o">&amp;&amp;</span> <span class="n">ii</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">ii</span><span class="p">)</span> <span class="p">{</span>
                    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">jj</span> <span class="o">=</span> <span class="n">j</span><span class="p">;</span> <span class="n">jj</span> <span class="o">&lt;</span> <span class="n">j</span> <span class="o">+</span> <span class="n">blockSize</span> <span class="o">&amp;&amp;</span> <span class="n">jj</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="o">++</span><span class="n">jj</span><span class="p">)</span> <span class="p">{</span>
                        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">kk</span> <span class="o">=</span> <span class="n">k</span><span class="p">;</span> <span class="n">kk</span> <span class="o">&lt;</span> <span class="n">k</span> <span class="o">+</span> <span class="n">blockSize</span> <span class="o">&amp;&amp;</span> <span class="n">kk</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="o">++</span><span class="n">kk</span><span class="p">)</span> <span class="p">{</span>
                            <span class="n">C</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">jj</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">ii</span><span class="p">][</span><span class="n">kk</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">kk</span><span class="p">][</span><span class="n">jj</span><span class="p">];</span>
                        <span class="p">}</span>
                    <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">C</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Disappointing Results</strong>:</p> <ul> <li>Block size 8: <strong>0.469667 TFLOPS</strong> </li> <li>Block size 16: <strong>0.499444 TFLOPS</strong> </li> <li>Block size 32: <strong>0.459444 TFLOPS</strong> </li> </ul> <p><strong>Lesson Learned</strong>: Blocking didn’t significantly improve performance and sometimes hurt it. This taught me that not all optimizations work in all contexts, and that the overhead of nested loops can sometimes outweigh cache benefits for certain problem sizes.</p> <h2 id="task-3-gpu-programming-with-cuda">Task 3: GPU Programming with CUDA</h2> <p>Moving from CPU to GPU programming represents a fundamental shift in thinking. Instead of optimizing for a few powerful cores, one must think about thousands of lightweight threads working in parallel on an <strong>RTX 6000 GPU</strong>. Boundary conditions are critical and everything must be thought of in terms of collective operations (e.g. <code class="language-plaintext highlighter-rouge">all_reduce</code>). We do this with special cuda primities such as <code class="language-plaintext highlighter-rouge">__shfl_down_sync</code>.</p> <h4 id="understanding-the-gpu-memory-hierarchy-and-roofline-model">Understanding the GPU Memory Hierarchy and Roofline Model</h4> <p>The GPU introduces a much more complex memory hierarchy and computational model. See figure 1 for this in a basic roofline representation.</p> <p><strong>GPU Roofline Analysis</strong>:</p> <ul> <li> <strong>Peak Performance</strong>: 133632 GF/s (Tensor), 16691 GF/s (Single-Precision)</li> <li> <strong>Memory Bandwidth</strong>: ~760 GB/s</li> <li> <strong>Arithmetic Intensity</strong>: Still 1/24 FLOPS/byte (memory-bound)</li> </ul> <p>The dramatically higher computational capacity meant that memory optimization would be even more critical.</p> <h4 id="thread-organization-understanding-warps-blocks-and-grids">Thread Organization: Understanding Warps, Blocks, and Grids</h4> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">#define WARP_SIZE 32
#define FULL_MASK 0xffffffff
</span></code></pre></div></div> <p><strong>Key Concept</strong>: GPU threads are organized hierarchically:</p> <ul> <li> <strong>Threads</strong> execute in groups of 32 called <strong>warps</strong> </li> <li> <strong>Warps</strong> execute in lockstep (SIMT - Single Instruction, Multiple Thread)</li> <li> <strong>Blocks</strong> contain multiple warps and share fast shared memory</li> <li> <strong>Grids</strong> contain multiple blocks distributed across the GPU</li> </ul> <h4 id="baseline-implementation-one-thread-per-row">Baseline Implementation: One Thread Per Row</h4> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span>
<span class="kt">void</span> <span class="nf">mvKernelNoWarp</span><span class="p">(</span><span class="kt">double</span><span class="o">*</span> <span class="n">matrix</span><span class="p">,</span> <span class="kt">double</span><span class="o">*</span> <span class="n">vector</span><span class="p">,</span>
              <span class="kt">int</span> <span class="n">numRowsM</span><span class="p">,</span> <span class="kt">int</span> <span class="n">vecLength</span><span class="p">,</span>
              <span class="kt">double</span><span class="o">*</span> <span class="n">result</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">numRowsM</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">vecLength</span><span class="p">;</span> <span class="n">col</span> <span class="o">++</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">vecLength</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">result</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">+=</span> <span class="n">matrix</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">vecLength</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">*</span> <span class="n">vector</span><span class="p">[</span><span class="n">col</span><span class="p">];</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Baseline Performance</strong>: <strong>0.000298438 TFLOPS</strong> - disappointing compared to CPU implementations.</p> <p><strong>Problem</strong>: This approach doesn’t leverage the GPU’s parallel execution model effectively. Each thread does sequential work, similar to CPU programming.</p> <h4 id="optimization-1-single-warp-per-row---learning-warp-level-programming">Optimization 1: Single Warp Per Row - Learning Warp-Level Programming</h4> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span>
<span class="kt">void</span> <span class="nf">mvKernelSingleWarp</span><span class="p">(</span><span class="kt">double</span><span class="o">*</span> <span class="n">matrix</span><span class="p">,</span> <span class="kt">double</span><span class="o">*</span> <span class="n">vector</span><span class="p">,</span>
              <span class="kt">int</span> <span class="n">numRowsM</span><span class="p">,</span> <span class="kt">int</span> <span class="n">vecLength</span><span class="p">,</span>
              <span class="kt">double</span><span class="o">*</span> <span class="n">result</span><span class="p">)</span> <span class="p">{</span>

  <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">lane</span> <span class="o">=</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span> <span class="o">%</span> <span class="n">WARP_SIZE</span><span class="p">;</span>
  <span class="kt">double</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">numRowsM</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Each thread in the warp handles different columns</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">lane</span><span class="p">;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">vecLength</span><span class="p">;</span> <span class="n">col</span> <span class="o">+=</span> <span class="n">WARP_SIZE</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">vecLength</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">sum</span> <span class="o">+=</span> <span class="n">matrix</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">vecLength</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">*</span> <span class="n">vector</span><span class="p">[</span><span class="n">col</span><span class="p">];</span>
      <span class="p">}</span>
    <span class="p">}</span>

    <span class="n">__syncwarp</span><span class="p">();</span>
    <span class="c1">// Warp reduction using shuffle instructions</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">WARP_SIZE</span><span class="o">/</span><span class="mi">2</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">sum</span> <span class="o">+=</span> <span class="n">__shfl_down_sync</span><span class="p">(</span><span class="n">FULL_MASK</span><span class="p">,</span> <span class="n">sum</span><span class="p">,</span> <span class="n">offset</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">result</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Performance</strong>: <strong>0.000346125 TFLOPS</strong> (slight improvement)</p> <p><strong>Breakthrough Concept</strong>: The <code class="language-plaintext highlighter-rouge">__shfl_down_sync</code> instruction is a warp-level primitive that allows threads within a warp to exchange data without using shared memory. This tree-reduction pattern:</p> <ol> <li>Thread 0 gets sum from thread 16</li> <li>Thread 0 gets sum from thread 8</li> <li>Thread 0 gets sum from thread 4</li> <li>Thread 0 gets sum from thread 2</li> <li>Thread 0 gets sum from thread 1</li> </ol> <p>In just 5 steps, we reduce 32 partial sums to a single result!</p> <h4 id="optimization-2-multiple-warps-per-row---hierarchical-reduction">Optimization 2: Multiple Warps Per Row - Hierarchical Reduction</h4> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__global__</span>
<span class="kt">void</span> <span class="nf">mvKernelMultipleWarps</span><span class="p">(</span><span class="kt">double</span><span class="o">*</span> <span class="n">matrix</span><span class="p">,</span> <span class="kt">double</span><span class="o">*</span> <span class="n">vector</span><span class="p">,</span>
              <span class="kt">int</span> <span class="n">numRowsM</span><span class="p">,</span> <span class="kt">int</span> <span class="n">vecLength</span><span class="p">,</span>
              <span class="kt">double</span><span class="o">*</span> <span class="n">result</span><span class="p">)</span> <span class="p">{</span>

  <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">lane</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">%</span> <span class="n">WARP_SIZE</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">warpid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="o">/</span><span class="n">WARP_SIZE</span><span class="p">;</span>
  <span class="kt">int</span> <span class="n">nwarps</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">/</span><span class="n">WARP_SIZE</span><span class="p">;</span>

  <span class="kt">double</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">;</span>

  <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&lt;</span> <span class="n">numRowsM</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Distribute work across multiple warps</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">col</span> <span class="o">=</span> <span class="n">lane</span> <span class="o">+</span> <span class="n">WARP_SIZE</span><span class="o">*</span><span class="n">warpid</span><span class="p">;</span> <span class="n">col</span> <span class="o">&lt;</span> <span class="n">vecLength</span><span class="p">;</span> <span class="n">col</span> <span class="o">+=</span> <span class="n">WARP_SIZE</span><span class="o">*</span><span class="n">nwarps</span><span class="p">)</span> <span class="p">{</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">col</span> <span class="o">&lt;</span> <span class="n">vecLength</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">sum</span> <span class="o">+=</span> <span class="n">matrix</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">vecLength</span> <span class="o">+</span> <span class="n">col</span><span class="p">]</span> <span class="o">*</span> <span class="n">vector</span><span class="p">[</span><span class="n">col</span><span class="p">];</span>
      <span class="p">}</span>
    <span class="p">}</span>

    <span class="n">__syncwarp</span><span class="p">();</span>
    <span class="c1">// Intra-warp reduction</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">WARP_SIZE</span><span class="o">/</span><span class="mi">2</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">sum</span> <span class="o">+=</span> <span class="n">__shfl_down_sync</span><span class="p">(</span><span class="n">FULL_MASK</span><span class="p">,</span> <span class="n">sum</span><span class="p">,</span> <span class="n">offset</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">__shared__</span> <span class="kt">double</span> <span class="n">s_mem</span><span class="p">[</span><span class="mi">1024</span><span class="o">/</span><span class="n">WARP_SIZE</span><span class="p">];</span> <span class="c1">// max 32 warps per block</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">s_mem</span><span class="p">[</span><span class="n">warpid</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="n">__syncthreads</span><span class="p">();</span> <span class="c1">// sync threads within block</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span> <span class="c1">// first lane in first warp</span>
      <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">nwarps</span><span class="p">;</span> <span class="o">++</span><span class="n">j</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">result</span><span class="p">[</span><span class="n">row</span><span class="p">]</span> <span class="o">+=</span> <span class="n">s_mem</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Dramatic Performance Improvement</strong>: <strong>0.00972644 TFLOPS</strong> with 32 warps per row - a <strong>32x improvement</strong> over the baseline!</p> <p><strong>Advanced Concepts Learned</strong>:</p> <ul> <li> <strong>Hierarchical Reduction</strong>: First reduce within warps using shuffle instructions, then across warps using shared memory</li> <li> <strong>Memory Coalescing</strong>: Threads in a warp access contiguous memory locations (lane + WARP_SIZE*warpid ensures proper stride)</li> <li> <strong>Shared Memory</strong>: Fast on-chip memory shared by threads in the same block</li> <li> <strong>Synchronization</strong>: <code class="language-plaintext highlighter-rouge">__syncwarp()</code> for intra-warp sync, <code class="language-plaintext highlighter-rouge">__syncthreads()</code> for intra-block sync</li> </ul> <h4 id="performance-scaling-analysis">Performance Scaling Analysis</h4> <p>The performance gains were heavily dependent on matrix dimensions:</p> <table> <thead> <tr> <th>Matrix Size</th> <th>No Warp</th> <th>Single Warp</th> <th>Multiple Warps (32)</th> <th>Speedup</th> </tr> </thead> <tbody> <tr> <td>10×10</td> <td>4e-06</td> <td>0</td> <td>0</td> <td>N/A</td> </tr> <tr> <td>100×100</td> <td>4.5e-05</td> <td>4.6e-05</td> <td>0.000333</td> <td>7.4x</td> </tr> <tr> <td>1000×1000</td> <td>0.000237</td> <td>0.000294</td> <td>0.003174</td> <td>13.4x</td> </tr> <tr> <td>10000×10000</td> <td>0.000389</td> <td>0.000704</td> <td>0.03125</td> <td>80.3x</td> </tr> </tbody> </table> <p><strong>Key Insight</strong>: GPU performance scales dramatically with problem size. Small problems don’t have enough parallelism to saturate the GPU, while large problems can achieve spectacular speedups.</p> <h4 id="special-case-many-columns-few-rows">Special Case: Many Columns, Few Rows</h4> <p>For matrices with many columns but few rows (e.g., 10×10000), the multiple warps strategy achieved <strong>0.018181 TFLOPS</strong> - a <strong>171x improvement</strong> over the baseline 0.000106 TFLOPS. This scenario is ideal for GPU parallelization because each row has enough work to keep many threads busy.</p> <h2 id="task-4-cuda-streams">Task 4: CUDA Streams</h2> <h3 id="beyond-single-stream-execution">Beyond Single-Stream Execution</h3> <p>The final task introduced sophisticated execution strategies needed for production GPU code, focusing on <strong>concurrent execution</strong> and <strong>memory transfer optimization</strong>.</p> <h4 id="baseline-single-stream-performance">Baseline: Single Stream Performance</h4> <p>Using the optimized multiple-warps kernel from task 3 as a baseline:</p> <table> <thead> <tr> <th>Matrix Size</th> <th>Time (μs)</th> <th>FLOP Rate</th> </tr> </thead> <tbody> <tr> <td>1000×1000</td> <td>7,132</td> <td>0.000338 TFLOPS</td> </tr> <tr> <td>1500×1500</td> <td>7,004</td> <td>0.000512 TFLOPS</td> </tr> <tr> <td>2000×2000</td> <td>7,341</td> <td>0.001012 TFLOPS</td> </tr> </tbody> </table> <p><strong>Baseline Average</strong>: <strong>0.000620 TFLOPS</strong></p> <h4 id="cuda-streams-overlapping-computation-and-communication">CUDA Streams: Overlapping Computation and Communication</h4> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">matVecMul</span><span class="p">(</span><span class="kt">double</span><span class="o">*</span> <span class="n">mat_h</span><span class="p">,</span> <span class="kt">double</span><span class="o">*</span> <span class="n">vec_h</span><span class="p">,</span> <span class="kt">double</span><span class="o">*</span> <span class="n">result_h</span><span class="p">,</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">numRowsM</span><span class="p">,</span> <span class="kt">int</span> <span class="n">vecLength</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">double</span> <span class="o">*</span><span class="n">mat_d</span><span class="p">,</span> <span class="o">*</span><span class="n">vec_d</span><span class="p">,</span> <span class="o">*</span><span class="n">result_d</span><span class="p">;</span>

    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mat_d</span><span class="p">,</span> <span class="n">numRowsM</span> <span class="o">*</span> <span class="n">vecLength</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>
    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">vec_d</span><span class="p">,</span> <span class="n">vecLength</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>
    <span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">result_d</span><span class="p">,</span> <span class="n">numRowsM</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">));</span>

    <span class="kt">int</span> <span class="n">streamNumRows</span> <span class="o">=</span> <span class="p">(</span><span class="n">numRowsM</span> <span class="o">+</span> <span class="n">M</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">M</span><span class="p">;</span> <span class="c1">// ceil division</span>
    <span class="n">cudaStream_t</span> <span class="n">streams</span><span class="p">[</span><span class="n">M</span><span class="p">];</span>

    <span class="c1">// Create multiple streams for concurrent execution</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>

    <span class="c1">// Launch kernels on different streams</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">mvKernelMultipleWarps</span><span class="o">&lt;&lt;&lt;</span><span class="n">nblocks</span><span class="p">,</span> <span class="n">nthreads</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
            <span class="n">mat_d</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">streamNumRows</span> <span class="o">*</span> <span class="n">vecLength</span><span class="p">,</span>
            <span class="n">vec_d</span><span class="p">,</span>
            <span class="n">numRowsM</span><span class="p">,</span> <span class="n">vecLength</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">streamNumRows</span><span class="p">,</span>
            <span class="n">result_d</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">streamNumRows</span>
        <span class="p">);</span>
    <span class="p">}</span>

    <span class="c1">// Synchronize and cleanup</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
        <span class="n">cudaStreamDestroy</span><span class="p">(</span><span class="n">streams</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <h4 id="stream-performance-analysis">Stream Performance Analysis</h4> <p><strong>Performance Results with Multiple Streams</strong>:</p> <table> <thead> <tr> <th>Matrix Size</th> <th>Streams</th> <th>Time (μs)</th> <th>FLOP Rate</th> <th>Improvement</th> </tr> </thead> <tbody> <tr> <td>1000×1000</td> <td>1</td> <td>7,132</td> <td>0.000338</td> <td>Baseline</td> </tr> <tr> <td> </td> <td>2</td> <td>7,609</td> <td>0.000262</td> <td>-22.5%</td> </tr> <tr> <td> </td> <td>3</td> <td>2,451</td> <td>0.000815</td> <td>+141%</td> </tr> <tr> <td> </td> <td>4</td> <td>2,474</td> <td>0.000808</td> <td>+139%</td> </tr> <tr> <td> </td> <td>7</td> <td>2,426</td> <td>0.000824</td> <td>+144%</td> </tr> <tr> <td>1500×1500</td> <td>1</td> <td>7,004</td> <td>0.000512</td> <td>Baseline</td> </tr> <tr> <td> </td> <td>2</td> <td>3,796</td> <td>0.001185</td> <td>+131%</td> </tr> <tr> <td> </td> <td>8</td> <td>3,727</td> <td>0.001207</td> <td>+136%</td> </tr> <tr> <td>2000×2000</td> <td>1</td> <td>7,341</td> <td>0.001012</td> <td>Baseline</td> </tr> <tr> <td> </td> <td>3</td> <td>5,316</td> <td>0.001504</td> <td>+49%</td> </tr> <tr> <td> </td> <td>4</td> <td>4,829</td> <td>0.001656</td> <td>+64%</td> </tr> </tbody> </table> <p><strong>Overall Average</strong>: <strong>0.000956 TFLOPS</strong> - a <strong>54% improvement</strong> over single-stream execution.</p> <h4 id="critical-discovery-stream-creation-overhead">Critical Discovery: Stream Creation Overhead</h4> <p><strong>Stream Creation Timing Analysis</strong>:</p> <ul> <li>1 stream: <strong>17 μs</strong> average</li> <li>2 streams: <strong>21 μs</strong> average</li> <li>3 streams: <strong>23 μs</strong> average</li> <li>8 streams: <strong>33 μs</strong> average</li> </ul> <p><strong>Key Findings</strong>:</p> <ol> <li> <strong>Non-linear scaling</strong>: Stream creation overhead doesn’t scale linearly</li> <li> <strong>First stream penalty</strong>: Creating the first stream takes much longer than subsequent streams</li> <li> <strong>Diminishing returns</strong>: Performance plateaued after ~3 streams</li> </ol> <h4 id="memory-transfer-bottleneck-discovery">Memory Transfer Bottleneck Discovery</h4> <p>Through profiling analysis, I discovered that <strong>memory allocation and transfer operations took significantly longer than the actual computation</strong>. The total matrix-vector multiplication averaged only <strong>11.00800 μs</strong>, while memory operations dominated the execution time.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/cuda_programming/concurrent_smaller-480.webp 480w, /assets/img/cuda_programming/concurrent_smaller-800.webp 800w, /assets/img/cuda_programming/concurrent_smaller-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/cuda_programming/concurrent_smaller.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="nsight systems" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/cuda_programming/create_streams-480.webp 480w, /assets/img/cuda_programming/create_streams-800.webp 800w, /assets/img/cuda_programming/create_streams-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/cuda_programming/create_streams.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="cuda stream" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Top shows Nvidia Nsight Systems view of the concurrent computation and bottom shows stream creation. </div> <p><strong>Profiler Insights</strong>:</p> <ul> <li>Kernel execution and result vector copying happened <strong>concurrently</strong> across multiple streams</li> <li>Memory transfers from host to device were optimized by the NVIDIA driver to avoid redundant copies</li> <li>The concurrent execution pattern showed clear overlap between computation and communication</li> </ul> <h4 id="pinned-memory-optimization">Pinned Memory Optimization</h4> <p>All host memory was allocated using <code class="language-plaintext highlighter-rouge">cudaHostAlloc</code> for page-locked (pinned) memory, which enables:</p> <ul> <li> <strong>Faster transfers</strong>: Pinned memory can be transferred via DMA without CPU involvement</li> <li> <strong>Concurrent execution</strong>: Allows overlap of memory transfers with kernel execution</li> <li> <strong>Predictable performance</strong>: Eliminates page faults during transfers</li> </ul> <p>I was fairly surprised with how quickly the effect of stream concurrency leveled out. Additionally, it was interesting how much longer memory allocation/copying took than the matrix vector calculation. I think the best way to optimize the program going forward would be to optimize the memory allocation/freeing/copying, as that seems to create the heaviest burden computationally</p> <p>–</p> <h3 id="summary">Summary</h3> <h4 id="1-performance-analysis-frameworks">1. Performance Analysis Frameworks</h4> <ul> <li> <strong>Roofline Model</strong>: Understanding the theoretical performance bounds imposed by arithmetic intensity</li> <li> <strong>Memory Hierarchy Optimization</strong>: Cache behavior, memory access patterns, and bandwidth limitations</li> <li> <strong>Profiling and Measurement</strong>: Using timing instrumentation and GPU profilers to identify bottlenecks</li> </ul> <h4 id="2-parallel-algorithm-design-principles">2. Parallel Algorithm Design Principles</h4> <ul> <li> <strong>Decomposition Strategies</strong>: How to break problems into parallel components</li> <li> <strong>Load Balancing</strong>: Ensuring work is distributed evenly across processing units</li> <li> <strong>Synchronization</strong>: Managing dependencies and data races in parallel execution</li> <li> <strong>Scalability</strong>: Understanding how performance changes with problem size and hardware resources</li> </ul> <h4 id="3-hardware-software-co-design">3. Hardware-Software Co-design</h4> <ul> <li> <strong>CPU Optimizations</strong>: Cache-friendly algorithms, compiler optimizations, instruction-level parallelism</li> <li> <strong>GPU Architecture</strong>: Warp-based execution, memory coalescing, hierarchical memory systems</li> <li> <strong>Memory Management</strong>: Pinned memory, concurrent transfers, stream scheduling</li> </ul> <h4 id="4-advanced-gpu-programming-patterns">4. Advanced GPU Programming Patterns</h4> <ul> <li> <strong>Warp-level Programming</strong>: Using shuffle instructions for efficient intra-warp communication</li> <li> <strong>Hierarchical Reduction</strong>: Combining warp-level and block-level reduction strategies</li> <li> <strong>Stream Programming</strong>: Overlapping computation with communication for better resource utilization</li> </ul> <h3 id="tools-and-techniques">Tools and Techniques</h3> <p>Here are some of the tools and the techniques that I used for these experiments.</p> <h4 id="development-and-profiling-tools">Development and Profiling Tools</h4> <ul> <li> <strong>CUDA Toolkit</strong>: GPU programming, debugging, and optimization</li> <li> <strong>NVIDIA Nsight</strong>: Advanced GPU profiling and performance analysis</li> <li> <strong>OpenMP</strong>: CPU parallelization and thread management</li> <li> <strong>GCC Optimization Flags</strong>: Understanding compiler behavior and optimization levels</li> <li> <strong>SLURM</strong>: High-performance computing job scheduling and resource management</li> </ul> <h4 id="programming-patterns-and-algorithms">Programming Patterns and Algorithms</h4> <ul> <li> <strong>Reduction Operations</strong>: Tree-based algorithms for combining distributed results</li> <li> <strong>Memory Coalescing</strong>: Optimizing memory access patterns for GPU efficiency</li> <li> <strong>Warp-level Programming</strong>: Leveraging GPU architectural features for performance</li> <li> <strong>Stream Programming</strong>: Concurrent execution and resource scheduling</li> <li> <strong>Cache-blocking</strong>: Optimizing for memory hierarchy behavior</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>The progression from basic CPU optimizations to advanced GPU programming represents more than just a technical journey—it’s a fundamental transformation in computational thinking. Each task built systematically upon the previous, creating a comprehensive understanding of modern parallel computing systems.</p> <p><strong>Most Valuable Insights</strong>:</p> <ol> <li> <p><strong>Performance is multidimensional</strong>: It’s not just about faster algorithms, but about understanding the complex interplay between hardware architecture, memory systems, and algorithmic design.</p> </li> <li> <p><strong>Optimization requires systematic methodology</strong>: The combination of theoretical analysis (roofline models), empirical measurement (profiling), and iterative refinement proved essential for achieving significant performance gains.</p> </li> <li> <p><strong>Parallel programming demands new mental models</strong>: Moving from sequential to parallel thinking required learning about synchronization, load balancing, memory coherence, and hierarchical algorithm design.</p> </li> <li> <p><strong>Hardware evolution drives software innovation</strong>: As computing systems become more parallel and heterogeneous, understanding multiple programming models and optimization strategies becomes increasingly critical.</p> </li> </ol> <p>The 668x performance improvement from the initial CPU implementation to the final GPU+streams version demonstrates the transformative power of parallel computing. However, the real value lies not in the speedup numbers, but in developing the analytical skills, debugging methodologies, and architectural understanding needed to tackle future computational challenges.</p> <p>As computing continues to evolve toward exascale systems, quantum computers, and neuromorphic processors, the fundamental principles I believe will stay the same—understanding hardware, measuring performance, thinking in parallel, and designing for scalability—will remain the essential foundation for pushing the boundaries of what’s computationally possible.</p> <hr> <p><em>This blog post describes my learning journey through APMA2822 at Brown University. The complete code implementations, experimental data, and profiling results are available in the <a href="https://github.com/surajk610/APMA2822" rel="external nofollow noopener" target="_blank">course repository</a>. Disclosure: Parts of this blog post were written w/ Claude so please email me at firstnamek610 at gmail</em></p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Suraj Anand. Thank you for visiting. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>