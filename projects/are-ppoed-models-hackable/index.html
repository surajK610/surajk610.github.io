<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>hacking-gpt | Suraj Anand</title> <meta name="author" content="Suraj Anand"> <meta name="description" content="using mechinterp to hack a PPO-ed GPT-2"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?505194d008ab58abcc3c0db57c2ae876"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://surajk610.github.io/projects/are-ppoed-models-hackable/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Suraj Anand</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">hacking-gpt</h1> <p class="post-description">using mechinterp to hack a PPO-ed GPT-2</p> </header> <article> <h1 id="are-ppo-ed-language-models-hackable">Are PPO-ed Language Models Hackable?</h1> <p><em>Exploring how reinforcement learning “aligns” language models and whether this alignment can be circumvented through mechanistic understanding. This research was written in spring of 2024 (back when RLHF was still somewhat popular).</em></p> <h2 id="the-alignment-challenge">The Alignment Challenge</h2> <p>Large language models come with a fundamental problem: they often exhibit undesirable behaviors like toxicity, bias, and negative sentiment. This has led to the widespread adoption of <strong>Reinforcement Learning from Human Feedback (RLHF)</strong> and specifically <strong>Proximal Policy Optimization (PPO)</strong> to “align” these models with human preferences.</p> <p>The critical question we are interested in: <em>Are these alignment techniques actually removing harmful capabilities, or are they simply learning to hide them?</em></p> <p>Our research tackles this question head-on by examining GPT-2 through the lens of <strong>mechanistic interpretability</strong> before and after PPO training. What we discovered has important implications for AI safety and the robustness of current alignment methods. Recent research by <a href="https://arxiv.org/abs/2401.01967" rel="external nofollow noopener" target="_blank">Lee et al. (2024)</a> discovered that models aligned with Direct Preference Optimization (DPO) to avoid toxicity don’t actually remove toxic knowledge—they just learn an “offset” to avoid expressing it. We extend this finding to examine whether PPO exhibits similar behavior with sentiment generation.</p> <h2 id="a-controlled-study-of-sentiment-alignment">A Controlled Study of Sentiment Alignment</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/ppoed_models/ppo_actor_crit-480.webp 480w, /assets/img/ppoed_models/ppo_actor_crit-800.webp 800w, /assets/img/ppoed_models/ppo_actor_crit-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/ppoed_models/ppo_actor_crit.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Here is the actor-critic model that has been used for PPO. Specifically, we use </div> <p>Rather than studying the complex domain of toxicity directly, we designed a controlled experiment focusing on <strong>sentiment generation</strong>:</p> <ul> <li> <strong>Base Model</strong>: GPT-2 fine-tuned on the IMDB movie review dataset</li> <li> <strong>Alignment Target</strong>: Generate positive sentiment responses instead of negative ones</li> <li> <strong>Reward Model</strong>: A DistilBERT classifier trained to detect positive sentiment</li> <li> <strong>Experimentation</strong>: Full access to model weights and activations (white-box analysis)</li> </ul> <p>This setup allowed us to peer inside the model’s “mind” and understand exactly how PPO changes the underlying computations.</p> <h2 id="mechanistic-analysis">Mechanistic Analysis</h2> <p>We used mechanistic interpreptability techniques to conduct our analysis.</p> <h3 id="step-1-identifying-negative-weights">Step 1: Identifying “Negative” Weights</h3> <p>Using insights from recent mechanistic interpretability research, we identified which parts of GPT-2 are responsible for generating negative sentiment. Here’s how:</p> <ol> <li> <strong>Trained a sentiment probe</strong> on the model’s internal representations</li> <li> <strong>Found “negative” value vectors</strong> using cosine similarity with the probe weights</li> <li> <strong>Mapped these vectors to vocabulary space</strong> to see what concepts they represent</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">find_negative_weight_meanings</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">w_negative</span><span class="p">,</span> <span class="n">hooked_model</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_weights</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">scores_gpt2</span> <span class="o">=</span> <span class="nf">get_svd</span><span class="p">(</span><span class="n">hooked_model</span><span class="p">,</span> <span class="n">w_negative</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
    
    <span class="n">vectors_of_interest</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="n">_score_obj</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">_score_obj</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">_score_obj</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">_score_obj</span> <span class="ow">in</span> <span class="n">scores_gpt2</span><span class="p">[:</span><span class="mi">64</span><span class="p">]</span>
    <span class="p">]</span>
    
    <span class="n">topn_negative_value_weights</span> <span class="o">=</span> <span class="n">vectors_of_interest</span><span class="p">[:</span><span class="n">n_weights</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">topn_negative_value_weights</span><span class="p">:</span>
       <span class="nf">decode_topk</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">hooked_model</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</code></pre></div></div> <p><strong>What we found</strong>: Specific neurons in layers 6-9 consistently activated for negative concepts like “useless,” “disastrous,” “fail,” and “bad.”</p> <table> <thead> <tr> <th>Layer</th> <th>Index</th> <th>Top Negative Tokens</th> </tr> </thead> <tbody> <tr> <td>7</td> <td>2394</td> <td>useless, mediocre, worthless</td> </tr> <tr> <td>6</td> <td>2360</td> <td>unus, disastrous, deteriorated</td> </tr> <tr> <td>9</td> <td>3047</td> <td>negligible, diminished, fewer</td> </tr> <tr> <td>7</td> <td>2464</td> <td>fail, Fail, Wrong</td> </tr> </tbody> </table> <h3 id="step-2-ppo-training-results">Step 2: PPO Training Results</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/ppoed_models/sentiment_change-480.webp 480w, /assets/img/ppoed_models/sentiment_change-800.webp 800w, /assets/img/ppoed_models/sentiment_change-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/ppoed_models/sentiment_change.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Histogram of sentiment of responses in heldout test set pre and post PPO the GPT-2. </div> <p>After PPO training, our model successfully learned to generate positive sentiment:</p> <ul> <li> <strong>Original model</strong>: Average sentiment score of 0.27</li> <li> <strong>PPO-trained model</strong>: Average sentiment score of 0.80</li> </ul> <p>The alignment worked! But <em>how</em> did it work?</p> <h2 id="ppo-doesnt-remove-negative-weights">PPO Doesn’t Remove Negative Weights</h2> <p>Here’s where our investigation revealed something surprising about how PPO actually works:</p> <h3 id="finding-1-weights-are-barely-changed">Finding #1: Weights Are Barely Changed</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/ppoed_models/weight_change-480.webp 480w, /assets/img/ppoed_models/weight_change-800.webp 800w, /assets/img/ppoed_models/weight_change-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/ppoed_models/weight_change.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Weights are minimally changed by Proximal Policy Optimization of the full GPT-2 model. </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Calculate cosine similarity between original and PPO weights
</span><span class="n">sims</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">().</span><span class="nf">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="sh">'</span><span class="s">h.11.</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">name</span> <span class="ow">or</span> <span class="sh">'</span><span class="s">h.10.</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
        <span class="n">param_ppo</span> <span class="o">=</span> <span class="n">trainer</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">()[</span><span class="sh">'</span><span class="s">base_model.</span><span class="sh">'</span> <span class="o">+</span> <span class="n">name</span><span class="p">]</span>
        <span class="n">curr_sim</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">param_ppo</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">cpu</span><span class="p">()</span>
        <span class="n">sims</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">curr_sim</span><span class="p">))</span>
</code></pre></div></div> <p><strong>Result</strong>: Almost all weights maintained cosine similarity ≥ 0.9998 with their original values!</p> <h3 id="finding-2-negative-weights-remain-intact">Finding #2: Negative Weights Remain Intact</h3> <p>The “negative” weights we identified earlier? They were virtually unchanged after PPO training:</p> <table> <thead> <tr> <th>Layer</th> <th>Index</th> <th>Original Tokens</th> <th>PPO-ed Tokens</th> </tr> </thead> <tbody> <tr> <td>7</td> <td>2394</td> <td>useless, mediocre, worthless</td> <td>useless, mediocre, worthless</td> </tr> <tr> <td>6</td> <td>2360</td> <td>unus, disastrous, deteriorated</td> <td>unus, disastrous, deteriorated</td> </tr> <tr> <td>9</td> <td>3047</td> <td>negligible, diminished, fewer</td> <td>negligible, diminished, fewer</td> </tr> </tbody> </table> <p><strong>Implication</strong>: PPO didn’t remove the model’s knowledge of negative concepts—it just learned to avoid expressing them.</p> <h3 id="finding-3-activations-change-not-weights">Finding #3: Activations Change, Not Weights</h3> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/ppoed_models/shit_prediction-480.webp 480w, /assets/img/ppoed_models/shit_prediction-800.webp 800w, /assets/img/ppoed_models/shit_prediction-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/ppoed_models/shit_prediction.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/ppoed_models/activation_change-480.webp 480w, /assets/img/ppoed_models/activation_change-800.webp 800w, /assets/img/ppoed_models/activation_change-1400.webp 1400w, " sizes="95vw" type="image/webp"></source> <img src="/assets/img/ppoed_models/activation_change.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> (Left) Logit lens shows that negative concept activations scaled down across model residual stream (this is specifically for the word sh*t). (Right) The activation differences for the ten most ‘negative’ value vectors. </div> <p>Using the “logit lens” technique, we tracked how concepts flow through the model’s residual stream:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Track negative concept activations across layers
</span><span class="k">def</span> <span class="nf">logit_lens_analysis</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">negative_vectors</span><span class="p">,</span> <span class="n">prompts</span><span class="p">):</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">cfg</span><span class="p">.</span><span class="n">n_layers</span><span class="p">):</span>
        <span class="c1"># Project intermediate representations to vocabulary space
</span>        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">unembed</span><span class="p">(</span><span class="n">residual_stream</span><span class="p">[</span><span class="n">layer</span><span class="p">])</span>
        <span class="n">activations</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="nf">track_negative_concepts</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">negative_vectors</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">activations</span>
</code></pre></div></div> <p><strong>Discovery</strong>: Negative concepts were still computed internally but were systematically suppressed in the residual stream as information flowed through the model.</p> <h2 id="the-hack-exploiting-ppos-weakness">The Hack: Exploiting PPO’s Weakness</h2> <p>Armed with this mechanistic understanding, we designed a “jailbreak” to force the aligned model to generate negative sentiment:</p> <h3 id="activation-scaling-attack">Activation Scaling Attack</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">activation_scaling_hack</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">negative_vectors</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Scale up activations corresponding to negative value vectors
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">hook_fn</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="c1"># Identify negative concept activations
</span>        <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">negative_vectors</span><span class="p">:</span>
            <span class="k">if</span> <span class="nf">is_negative_activation</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
                <span class="c1"># Scale up by factor of 10
</span>                <span class="n">output</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">*=</span> <span class="n">scale_factor</span>
        <span class="k">return</span> <span class="n">output</span>
    
    <span class="c1"># Register hooks on MLP layers
</span>    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span>  <span class="c1"># Layers containing negative weights
</span>        <span class="n">model</span><span class="p">.</span><span class="n">blocks</span><span class="p">[</span><span class="n">layer</span><span class="p">].</span><span class="n">mlp</span><span class="p">.</span><span class="nf">register_forward_hook</span><span class="p">(</span><span class="n">hook_fn</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Result</strong>: By scaling the activations of negative value vectors by a factor of 10, we successfully “jailbroke” the aligned model:</p> <ul> <li> <strong>PPO-aligned model</strong>: 0.80 average sentiment score</li> <li> <strong>After activation scaling</strong>: 0.43 average sentiment score (back to negative!)</li> </ul> <h2 id="implications-what-this-means-for-ai-safety">Implications: What This Means for AI Safety</h2> <p>Our findings reveal a fundamental limitation in current alignment approaches:</p> <h3 id="1-alignment--capability-removal">1. <strong>Alignment ≠ Capability Removal</strong> </h3> <p>PPO doesn’t actually remove harmful capabilities—it learns a “wrapper” that avoids expressing them. The underlying knowledge remains intact and potentially exploitable.</p> <h3 id="2-white-box-vulnerability">2. <strong>White-Box Vulnerability</strong> </h3> <p>If an adversary has access to model weights (increasingly common with open-source models), they can potentially reverse-engineer alignment measures.</p> <h3 id="3-the-offset-problem">3. <strong>The Offset Problem</strong> </h3> <p>Similar to findings in other research, PPO learns an “offset” that shifts the model’s behavior without fundamentally altering its internal representations.</p> <h2 id="attempted-solution-regularization-based-mitigation">Attempted Solution: Regularization-Based Mitigation</h2> <p>We experimented with modifying the PPO objective to actively penalize negative weights:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Modified reward function
</span><span class="n">r_modified</span> <span class="o">=</span> <span class="n">r_original</span> <span class="o">-</span> <span class="n">λ</span><span class="err">₁</span> <span class="o">*</span> <span class="n">r_KL</span> <span class="o">+</span> <span class="n">λ</span><span class="err">₂</span> <span class="o">*</span> <span class="n">Σ</span><span class="p">(</span><span class="o">||</span><span class="n">w</span> <span class="o">-</span> <span class="n">w_original</span><span class="o">||</span><span class="p">)</span>
<span class="c1">#                                    negative weights
</span></code></pre></div></div> <p>Where:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">r_original</code>: Standard PPO reward (positive sentiment)</li> <li> <code class="language-plaintext highlighter-rouge">λ₁ * r_KL</code>: KL divergence penalty (standard)</li> <li> <code class="language-plaintext highlighter-rouge">λ₂ * Σ(||w - w_original||)</code>: Penalty for preserving negative weights</li> </ul> <p><strong>Challenge</strong>: Finding the right balance proved difficult:</p> <ul> <li> <strong>λ₂ too low</strong>: Negative weights remain unchanged</li> <li> <strong>λ₂ too high</strong>: Model loses coherent language generation capabilities</li> </ul> <h2 id="future-directions-toward-more-robust-alignment">Future Directions: Toward More Robust Alignment</h2> <p>Our research suggests several promising directions:</p> <h3 id="1-mechanistic-informed-alignment">1. <strong>Mechanistic-Informed Alignment</strong> </h3> <ul> <li>Use interpretability tools to identify and specifically target harmful representations</li> <li>Design training objectives that explicitly modify problematic weights</li> </ul> <h3 id="2-adversarial-robustness-testing">2. <strong>Adversarial Robustness Testing</strong> </h3> <ul> <li>Systematically test aligned models against mechanistic attacks</li> <li>Develop benchmarks for alignment robustness beyond behavioral evaluation</li> </ul> <h3 id="3-fundamental-architecture-changes">3. <strong>Fundamental Architecture Changes</strong> </h3> <ul> <li>Explore architectures that make harmful capabilities truly removable</li> <li>Investigate whether different training paradigms avoid the “offset” problem</li> </ul> <h2 id="conclusion-the-road-ahead">Conclusion: The Road Ahead</h2> <p>Our study of PPO-aligned GPT-2 reveals that current alignment techniques may be more fragile than they appear. While PPO successfully changes surface behavior, it leaves underlying capabilities largely intact—creating potential vulnerabilities that could be exploited by sophisticated adversaries.</p> <p>This doesn’t mean current alignment research is worthless, but it does suggest we need:</p> <ol> <li> <strong>More robust evaluation methods</strong> that go beyond behavioral testing</li> <li> <strong>Mechanistic understanding</strong> of how alignment techniques actually work</li> <li> <strong>Adversarial red-teaming</strong> to identify potential failure modes</li> <li> <strong>Fundamental advances</strong> in alignment techniques that address root causes</li> </ol> <p>As systems become more powerful, understanding these fundamental limitations becomes increasingly critical. The path to truly safe and aligned AI will require not just better training techniques, but a deeper understanding of how these techniques actually change the models we’re trying to align.</p> <hr> <p><em>The complete codebase and experimental details are available on <a href="https://github.com/surajK610/rl-gpt2-sentiment" rel="external nofollow noopener" target="_blank">GitHub</a>. This work was conducted in collaboaration with David Getzen.</em></p> <h2 id="technical-appendix">Technical Appendix</h2> <p><strong>Mechanistic Interpretability:</strong></p> <ul> <li> <strong>Logit Lens</strong>: Project intermediate representations to vocabulary space</li> <li> <strong>Value Vector Analysis</strong>: Identify which neurons promote specific concepts</li> <li> <strong>Activation Patching</strong>: Modify internal activations to test causal effects</li> </ul> <p><strong>Training Configuration:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">PPOConfig</span><span class="p">(</span>
    <span class="n">num_rollouts</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">ppo_epochs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">init_kl_coef</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
    <span class="n">cliprange</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">gen_kwargs</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div> <p><strong>Evaluation Metrics:</strong></p> <ul> <li>Sentiment classification scores (DistilBERT-IMDB)</li> <li>Weight cosine similarity analysis</li> <li>Activation magnitude tracking</li> <li>Token-level negative concept detection</li> </ul> <h3 id="reproducibility">Reproducibility</h3> <p>All experiments were conducted using the <code class="language-plaintext highlighter-rouge">trlx</code> library for RLHF training and <code class="language-plaintext highlighter-rouge">transformer_lens</code> for mechanistic interpretability analysis.</p> <h3 id="references">References</h3> <p>Please see the <a href="https://surajk610.github.io/assets/pdf/Is_PPO_Hackable.pdf">preprint</a> for the full references.</p> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Suraj Anand. Thank you for visiting. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>